{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEUh9DJjfZ1o"
      },
      "outputs": [],
      "source": [
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5V9rYxAiIEZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6ZVlD4hiKCM"
      },
      "outputs": [],
      "source": [
        "class SentimentCorpus:\n",
        "    \n",
        "    def __init__(self, train_per=0.8, dev_per=0, test_per=0.2):\n",
        "        '''\n",
        "        prepare dataset\n",
        "        1) build feature dictionaries\n",
        "        2) split data into train/dev/test sets \n",
        "        '''\n",
        "        X, y, feat_dict, feat_counts = build_dicts()\n",
        "        self.nr_instances = y.shape[0]\n",
        "        self.nr_features = X.shape[1]\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feat_dict = feat_dict\n",
        "        self.feat_counts = feat_counts\n",
        "        \n",
        "        train_y, dev_y, test_y, train_X, dev_X, test_X = split_train_dev_test(self.X, self.y, train_per, dev_per, test_per)\n",
        "        self.train_X = train_X\n",
        "        self.train_y = train_y\n",
        "        self.dev_X = dev_X\n",
        "        self.dev_y = dev_y\n",
        "        self.test_X = test_X\n",
        "        self.test_y = test_y\n",
        "\n",
        "def split_train_dev_test(X, y, train_per, dev_per, test_per):\n",
        "    if (train_per + dev_per + test_per) > 1:\n",
        "        print(\"train/dev/test splits should sum to one\")\n",
        "        return\n",
        "    dim = y.shape[0]\n",
        "    split1 = int(dim * train_per)\n",
        "    \n",
        "    if dev_per == 0:\n",
        "        train_y, test_y = np.vsplit(y, [split1])\n",
        "        dev_y = np.array([])\n",
        "        train_X = X[0:split1,:]\n",
        "        test_X = X[split1:,:]\n",
        "        dev_X = np.array([])\n",
        "    else:\n",
        "        split2 = int(dim*(train_per+dev_per))\n",
        "        train_y,dev_y,test_y = np.vsplit(y,(split1,split2))\n",
        "        train_X = X[0:split1,:]\n",
        "        dev_X = X[split1:split2,:]\n",
        "        test_X = X[split2:,:]\n",
        "        \n",
        "    return train_y,dev_y,test_y,train_X,dev_X,test_X\n",
        "\n",
        "def build_dicts():\n",
        "    '''\n",
        "    builds feature dictionaries\n",
        "    ''' \n",
        "    feat_counts = {}\n",
        "\n",
        "    # build feature dictionary with counts\n",
        "    nr_pos = 0\n",
        "    with codecs.open(\"/content/positive.review\", 'r', 'utf8') as pos_file:\n",
        "        for line in pos_file:\n",
        "            nr_pos += 1\n",
        "            toks = line.split(\" \")\n",
        "            for feat in toks[0:-1]:\n",
        "                name, counts = feat.split(\":\")\n",
        "                if name not in feat_counts:\n",
        "                    feat_counts[name] = 0\n",
        "                feat_counts[name] += int(counts)\n",
        "    \n",
        "    nr_neg = 0\n",
        "    with codecs.open(\"/content/negative.review\", 'r', 'utf8') as neg_file:\n",
        "        for line in neg_file:\n",
        "            nr_neg += 1\n",
        "            toks = line.split(\" \")\n",
        "            for feat in toks[0:-1]:\n",
        "                name, counts = feat.split(\":\")\n",
        "                if name not in feat_counts:\n",
        "                    feat_counts[name] = 0\n",
        "                feat_counts[name] += int(counts)\n",
        "\n",
        "    # remove all features that occur less than 5 (threshold) times\n",
        "    to_remove = []\n",
        "    for key, value in feat_counts.items():\n",
        "        if value < 5:\n",
        "            to_remove.append(key)\n",
        "    for key in to_remove:\n",
        "        del feat_counts[key]\n",
        "\n",
        "    # map feature to index\n",
        "    feat_dict = {}\n",
        "    i = 0\n",
        "    for key in feat_counts.keys():\n",
        "        feat_dict[key] = i\n",
        "        i += 1\n",
        "\n",
        "    nr_feat = len(feat_counts) \n",
        "    nr_instances = nr_pos + nr_neg\n",
        "    X = np.zeros((nr_instances, nr_feat), dtype=float)\n",
        "    y = np.vstack((np.zeros([nr_pos,1], dtype=int), np.ones([nr_neg,1], dtype=int)))\n",
        "    \n",
        "    with codecs.open(\"/content/positive.review\", 'r', 'utf8') as pos_file:\n",
        "        nr_pos = 0\n",
        "        for line in pos_file:\n",
        "            toks = line.split(\" \")\n",
        "            for feat in toks[0:-1]:\n",
        "                name, counts = feat.split(\":\")\n",
        "                if name in feat_dict:\n",
        "                    X[nr_pos,feat_dict[name]] = int(counts)\n",
        "            nr_pos += 1\n",
        "        \n",
        "    with codecs.open(\"/content/negative.review\", 'r', 'utf8') as neg_file:\n",
        "        nr_neg = 0\n",
        "        for line in neg_file:\n",
        "            toks = line.split(\" \")\n",
        "            for feat in toks[0:-1]:\n",
        "                name, counts = feat.split(\":\")\n",
        "                if name in feat_dict:\n",
        "                    X[nr_pos+nr_neg,feat_dict[name]] = int(counts)\n",
        "            nr_neg += 1\n",
        "    \n",
        "    # shuffle the order, mix positive and negative examples\n",
        "    new_order = np.arange(nr_instances)\n",
        "    np.random.seed(0) # set seed\n",
        "    np.random.shuffle(new_order)\n",
        "    X = X[new_order,:]\n",
        "    y = y[new_order,:]\n",
        "    \n",
        "    return X, y, feat_dict, feat_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0rjcQf4RGRT"
      },
      "outputs": [],
      "source": [
        "a = SentimentCorpus()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_xrCAH8wpQ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "glove_vecs = {}\n",
        "\n",
        "#importing the static pretrained dataset\n",
        "\n",
        "with open(\"/content/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        word, vec = line.split(maxsplit=1)\n",
        "        glove_vecs[word] = np.fromstring(vec, sep=\" \") #splitting based on whitespace\n",
        "\n",
        "corpus_words = set(a.feat_dict.keys()) #importing the feature dictionary \n",
        "glove_words = set(glove_vecs.keys()) #the dictionary of words with the weightage\n",
        "\n",
        "words_not_present = list(corpus_words - glove_words) #list of words in the dataset not in glove dataset\n",
        "\n",
        "for word in a.feat_dict.keys():\n",
        "    if word not in glove_vecs.keys():\n",
        "        sub_words = word.split(\"_\")#splitting the hyphenated words that are not present in the glove dataset\n",
        "        for sub_word in sub_words:\n",
        "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
        "            if sub_word not in glove_vecs.keys():\n",
        "                print(sub_word)\n",
        "\n",
        "embedding_dict = {}\n",
        "\n",
        "for word in a.feat_dict.keys():\n",
        "    if word in glove_vecs:\n",
        "        embedding_dict[word] = glove_vecs[word] #creating an embedding dictionary for the words in the dataset\n",
        "    else:\n",
        "        emb = np.zeros(300)\n",
        "        sub_words = word.split(\"_\")\n",
        "        emb_vecs = []\n",
        "        for sub_word in sub_words:\n",
        "            sub_word = sub_word.replace(\"'\", \"\") # Remove apostrophes\n",
        "            if sub_word in glove_vecs:\n",
        "                emb_vecs.append(glove_vecs[sub_word])\n",
        "        if len(emb_vecs) > 0:\n",
        "            emb_vecs = np.array(emb_vecs)\n",
        "            emb = np.mean(emb_vecs, axis=0)\n",
        "        embedding_dict[word] = emb\n",
        "\n",
        "#mean pooling\n",
        "def convert_data_embedding(data):\n",
        "    converted_data = []\n",
        "    for datum in data:\n",
        "        count = 0\n",
        "        sent_emb_list = []\n",
        "        for word in a.feat_dict:\n",
        "            if word in embedding_dict:\n",
        "                sent_emb_list.append(datum[a.feat_dict[word]]*embedding_dict[word])\n",
        "                count += datum[a.feat_dict[word]]\n",
        "        if count > 0:\n",
        "            sent_emb_list = np.array(sent_emb_list)\n",
        "            final_emb = np.einsum(\"ij->j\",sent_emb_list)\n",
        "            final_emb = final_emb / count\n",
        "        else:\n",
        "            final_emb = np.zeros(50)\n",
        "        converted_data.append(final_emb)\n",
        "    converted_data = np.array(converted_data)\n",
        "    return converted_data\n",
        "\n",
        "#new train and test data with the embedding\n",
        "train_x_emb = convert_data_embedding(a.train_X)\n",
        "test_x_emb = convert_data_embedding(a.test_X)\n",
        "\n",
        "#using three layer mlp classifier with hidden unit layer 100 and activation function relu\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(100,100,100),activation='relu', random_state=0, solver= 'adam')\n",
        "nn_model.fit(train_x_emb, a.train_y.ravel())\n",
        "predictions = nn_model.predict(test_x_emb)\n",
        "acc= accuracy_score(a.test_y.ravel(),predictions)\n",
        "p= precision_score(a.test_y.ravel(),predictions)\n",
        "r = recall_score(a.test_y.ravel(),predictions)\n",
        "f1 = f1_score(a.test_y.ravel(),predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae6Ml5sQpJXG",
        "outputId": "08ec7c16-0a92-4de2-debd-0013b72429d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "(e.g.\n",
            "<year>\n",
            "<year>\n",
            "<num>\n",
            "<num>\n",
            "&quot;the\n",
            "<num>\n",
            "<year>\n",
            "koontzs\n",
            "koontzs\n",
            "\n",
            "(p.\n",
            "(p.\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "(im\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "\n",
            "<dash-num>\n",
            "youve\n",
            "youve\n",
            "\n",
            "<num>\n",
            "<year>\n",
            "<year>\n",
            "\"passing\"\n",
            "calvinos\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<year>\n",
            "<year>\n",
            "\n",
            "&quot;\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<fraction>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "grishams\n",
            "<num>\n",
            "<num>\n",
            "<year>\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "whove\n",
            "<num>\n",
            "\n",
            "<num>\n",
            "mcculloughs\n",
            "<num>\n",
            "\"one\"\n",
            "<year>\n",
            "<num>\n",
            "<year>\n",
            "\n",
            "book.this\n",
            "book.this\n",
            "story.the\n",
            "<num>\n",
            "<num>\n",
            "<year>\n",
            "&quot;must\n",
            "&quot;must\n",
            "have&quot;\n",
            "have&quot;\n",
            "salems\n",
            "<year>\n",
            "salems\n",
            "hadnt\n",
            "hadnt\n",
            "hiassins\n",
            "<num>\n",
            "hiassin\n",
            "<num>\n",
            "<num>\n",
            "(i.e.\n",
            "\n",
            "<num>\n",
            "werent\n",
            "\n",
            "<num>\n",
            "<fraction>\n",
            "youve\n",
            "\n",
            "<num>\n",
            "youve\n",
            "\n",
            "<num>\n",
            "everyones\n",
            "familys\n",
            "hiaasens\n",
            "<year>\n",
            "<num>\n",
            "it.i\n",
            "shouldnt\n",
            "theyve\n",
            "twains\n",
            "heinleins\n",
            "thorby\n",
            "youve\n",
            "<year>\n",
            "<num>\n",
            "(pp.\n",
            "(pp.\n",
            "<num>\n",
            "<num>\n",
            "<num>\n",
            "rosss\n",
            "explaination\n",
            "<num>\n",
            "book.i\n",
            "<num>\n",
            "<year>\n",
            "youve\n",
            "<num>\n",
            "cashs\n",
            "\n",
            "<num>\n",
            "marxs\n",
            "<num>\n",
            "<num>\n",
            "falwells\n",
            "lyddie\n",
            "<num>\n",
            "\n",
            "shreave\n",
            "\"mystery\"\n",
            "<num>\n",
            "\n",
            "theyd\n",
            "<num>\n",
            "schanzkowska\n",
            "<year>\n",
            "\n",
            "<num>\n",
            "\n",
            "hillermans\n",
            "hayeks\n",
            "\n",
            "kagans\n",
            "narby\n",
            "reinharts\n",
            "anyones\n",
            "intimancies\n",
            "intimancies\n",
            "jimis\n",
            "<fraction>\n",
            "\n",
            "jefferson-hemings\n",
            "proakis\n",
            "styrons\n",
            "cofee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy score: \",acc) \n",
        "print(\"precision score:\" ,p)\n",
        "print(\"recall score:\", r)\n",
        "print(\"f1 score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLOrFXWwrCv9",
        "outputId": "db8795e5-245e-431a-9fb7-062cd04a61b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score:  0.77\n",
            "precision score: 0.7526315789473684\n",
            "recall score: 0.7606382978723404\n",
            "f1 score: 0.7566137566137566\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}